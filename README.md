# Machine-Translation-by-Transformers-using-PyTorch
# Research paper Concept:
                Paper is dealt with innovative transformer architecture in depth.Here certain minor changes has been made as per the explanation from "Rishaap".Positional Embedding layer is not static embeddings and token embeddings are multiplied with scale value from hidden dimension.Training parameters are customized.
  ![image](https://user-images.githubusercontent.com/82649993/124318293-0deac880-db96-11eb-8ff1-df33c8b019b9.png)
              
# Multi30K dataset of German to English seq2seq liquistic data is chosen.
![image](https://user-images.githubusercontent.com/82649993/124318407-3b377680-db96-11eb-8d53-5fdb09b5ae63.png)

# Step by Step Implementation:
                 1.Text screening
                 2.Transformer Model Building
                 3.Training Parameters
                 4.Training Loop
                 5.Validation
                 6.Ticker and Bleu Score

# About loss , Perplexity and Bleu score
![image](https://user-images.githubusercontent.com/82649993/124318741-be58cc80-db96-11eb-9e81-6d4369488302.png)
![image](https://user-images.githubusercontent.com/82649993/124318795-d3cdf680-db96-11eb-9e58-db7960462282.png)
![image](https://user-images.githubusercontent.com/82649993/124318896-00820e00-db97-11eb-9db7-afeb525d677d.png)

# Test Example Data Ticker - Single head Attention visualization:
![image](https://user-images.githubusercontent.com/82649993/124319105-5ce52d80-db97-11eb-8a56-154237d3e7a9.png)

# Heavy PyTorch codes are practiced.
